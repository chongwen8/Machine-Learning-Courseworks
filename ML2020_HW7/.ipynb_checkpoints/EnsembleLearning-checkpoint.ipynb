{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "集成学习的思想很简单，就是构建多个学习器一起结合来完成具体的学习任务。通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛华性能，对弱学习器尤为明显。（三个臭皮匠，顶个诸葛亮）\n",
    "\n",
    "弱学习器是指学习器的学习正确率仅比随机猜测略好，强学习器是指学习器的学习正确率很高。集成学习就是结合多个弱分类器组成一个强分类器。\n",
    "\n",
    "集成学习可以分成两类：\n",
    "\n",
    "- 个体学习器间存在强依赖关系，必须串行生成学习模型。代表：Boosting（AdaBoost, Gradient Boosting Machine）。\n",
    "- 个体学习器间不存在强依赖关系，可同时生成学习模型。代表：Bagging和随机森林（Random Forest）。\n",
    "\n",
    "AdaBoost通过改变训练数据的权值来训练一组弱分类器，把这些弱分类器线性组合成为一个强分类器。GBDT结合提升树模型和梯度提升的优点，使用新弱分类器拟合前一次迭代模型的样本余量，逐渐降低训练误差。Bagging和随机森林利用自助采样采集T组训练样本集，分别训练T个分类器，对T个分类器的预测结果进行投票决定模型的最终预测结果。\n",
    "本次作业我们要实现adaboost, GBDT和Random Forest三个模型，为方便模型编写，我们采用scikit-learn构建CART树作为弱学习器。此外，添加了一个实战机器学习任务，Kaggle经典竞赛Titanic生存预测，通过这个任务大家可以比较不同集成学习算法的优劣。\n",
    "\n",
    "提示：scikit-learn是一个简单而有效的python机器学习算法库，里面包含了许多常见的机器学习算法（包括本课程讲的算法）。这里直接使用scikit-learn实现的CART算法，方便我们完成实验。点击[这里](http://scikit-learn.org/stable/)查看scikit-learn的官方文档，点击[这里](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)查看scikit-learn实现的CART算法的API接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的包\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **任务1：**构建一个简单的二分类数据集，了解并使用scikit-learn的DecisionTreeClassifier模块快速构建CART树并拟合数据集。\n",
    "\n",
    "请参考scikit-learn官方文档\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用scikit-learn生成一个简单的二分类数据集\n",
    "X, y = make_circles(n_samples=100, noise=0.5, factor=0.2, random_state=1)\n",
    "\n",
    "# 二分类标签一般是‘0’和‘1’，adaboost算法的标签为‘1’，‘-1’，修改adaboost标签‘0’变为‘-1’\n",
    "y_ada = y.copy()\n",
    "y_ada[y_ada == 0] = -1\n",
    "\n",
    "# 可视化生成的数据集\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先使用CART对数据进行训练\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# 要求采用DecisionTreeClassifier构建最大深度为5，其余为默认参数的决策树模型，并使用fit方法拟合生成的(X, y)数据，采用score方法测试模型精度\n",
    "cart_model = None\n",
    "None\n",
    "accuracy_cart = None\n",
    "print(\"Accuracy of CART model is {}\".format(accuracy_cart))\n",
    "\n",
    "\n",
    "# 可视化CART的分类效果\n",
    "x1_grid = np.linspace(-2, 2, 100)\n",
    "x2_grid = np.linspace(-2, 2, 100)\n",
    "\n",
    "x1_grid, x2_grid = np.meshgrid(x1_grid, x2_grid)\n",
    "y_grid_simple = np.zeros_like(x1_grid)\n",
    "\n",
    "X_grid = np.hstack([x1_grid.reshape(-1, 1), x2_grid.reshape(-1, 1)])\n",
    "\n",
    "# 采用predict方法使用生成模型进行分类预测\n",
    "y_grid = None\n",
    "\n",
    "y_grid_simple = y_grid.reshape(100, 100)\n",
    "\n",
    "### END CODE HERE ###\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.contourf(x1_grid, x2_grid, y_grid_simple, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "提升方法是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器。对于提升方法来说，有两个问题需要回答：一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。关于第1个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。至于第2个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减少分类误差率大的弱分类器的权值，使其在表决中起较少的作用。\n",
    "\n",
    "假设给定一个二分类的训练数据集\n",
    "$$\n",
    "T = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}\n",
    "$$\n",
    "其中，每个样本点由实例与标记组成。实例$x_i \\in \\mathcal{X} \\subseteq R^n$，$y_i \\in \\mathcal{Y} = \\{-1, +1\\}$，$\\mathcal{X}$是实例空间，$\\mathcal{Y}$是标记集合。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成为一个强分类器。\n",
    "\n",
    "输入：训练数据集$T=\\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}$，其中$x_i \\in \\mathcal{X} \\subseteq R^n$，$y_i \\in \\mathcal{Y} = \\{-1, +1\\}$；  \n",
    "输出：最终分类器$G(x)$。  \n",
    "(1)初始化训练数据的权值分布\n",
    "$$\n",
    "D_1 = (w_{11}, \\cdots, W_{1i}, \\cdots, W_{1N}), w_{1i} = \\frac{1}{N}, i = 1, 2, \\cdots, N\n",
    "$$\n",
    "(2)对$m=1,2,\\cdots,M$  \n",
    "(a)使用具有权值分布$D_m$的训练数据集学习，得到基本分类器\n",
    "$$\n",
    "G_m(x):\\mathcal{X}\\to \\{-1, +1\\}\n",
    "$$\n",
    "(b)计算$G_m(x)$在训练数据集上的分类误差率\n",
    "$$\n",
    "e_m = \\sum_{i=1}^{N} P(G_m(x_i) \\not = y_i) = \\sum_{i=1}^{N} w_{mi} I(G_m(x) \\not = y_i)\n",
    "$$\n",
    "(c)计算$G_m(x)$的系数\n",
    "$$\n",
    "\\alpha_m = \\frac{1}{2} \\log \\frac{1-e_m}{e_m} \\tag{1}\n",
    "$$\n",
    "(d)更新训练数据集的权值分布\n",
    "$$\n",
    "D_{m+1} = (w_{m+1,1}, \\cdots, w_{m+1,i}, \\cdots, w_{m+1,N})\n",
    "$$\n",
    "$$\n",
    "w_{m+1, i} = \\frac{w_{mi}}{Z_m} \\exp (-\\alpha_m y_i G_m(x_i))), i = 1, 2, \\cdots, N \\tag{2}\n",
    "$$\n",
    "这里，$Z_m$是规范化因子\n",
    "$$\n",
    "Z_m = \\sum_{i=1}^{N} w_{mi} \\exp(-\\alpha_m y_i G_m(x_i))\n",
    "$$\n",
    "它使$D_{m+1}$成为一个概率分布。  \n",
    "(3)构建基本分类器的线性组合\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M} \\alpha_m G_m(x)\n",
    "$$\n",
    "得到最终分类器\n",
    "$$\n",
    "G(x) = sign(f(x)) = sign \\left( \\sum_{m=1}^{M} \\alpha_m G_m(x) \\right)\n",
    "$$\n",
    "到此，算法结束。\n",
    "\n",
    "观察公式（1）$e_m$-$\\alpha_m$的曲线图\n",
    "\n",
    "![e_m-alpha_m](./images/image1.png)\n",
    "\n",
    "可以看到，当分类误差率低时，$\\alpha_m$的值较高，当分类误差率高时，$\\alpha_m$的值较低。\n",
    "\n",
    "来考虑一下，$e_m$是否会取0或1。如果$e_m = 0$，说明弱分类器的效果非常好，正确率100%，可以停止迭代了！如果$e_m = 1$，说明弱分类器的对所有样本都分类错误，学习出一个效果最差的分类器， 但这种情况出现的概率微乎其微！\n",
    "\n",
    "训练数据的权值分布的更新公式（2）可以写成：\n",
    "$$\n",
    "w_{m+1,i} = \\begin{cases} \\frac{w_{mi}}{Z_m}e^{-\\alpha_m}, & G_m(x_i) = y_i \\\\ \\frac{w_{mi}}{Z_m}e^{\\alpha_m}, & G_m(x_i) \\not = y_i \\end{cases}\n",
    "$$\n",
    "由此可知，被基本分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类样本的权值得以缩小。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **任务2：**根据上述描述构建adaboost算法\n",
    "\n",
    "弱学习器采用scikit-learn的DecisionTreeClassifier实现，其中决策树的深度可由输入参数控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(X, y, M, max_depth=None):\n",
    "    \"\"\"\n",
    "    adaboost函数，使用CART作为弱分类器\n",
    "    参数:\n",
    "        X: 训练样本\n",
    "        y: 样本标签, y = {-1, +1}\n",
    "        M: 使用M个弱分类器\n",
    "        max_depth: 基学习器CART决策树的最大深度\n",
    "    返回:\n",
    "        F: 生成的模型\n",
    "    \"\"\"\n",
    "    num_X, num_feature = X.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # 初始化训练数据的权值分布\n",
    "    D = None\n",
    "    \n",
    "    G = []\n",
    "    alpha = []\n",
    "    \n",
    "    for m in range(M):\n",
    "        # 使用具有权值分布D_m的训练数据集学习，得到基本分类器\n",
    "        # 使用DecisionTreeClassifier，设置树深度为max_depth\n",
    "        G_m = None\n",
    "        # 开始训练\n",
    "        None\n",
    "        # 计算G_m在训练数据集上的分类误差率\n",
    "        y_pred = None\n",
    "        e_m = None\n",
    "        \n",
    "        if e_m == 0:\n",
    "            break\n",
    "        \n",
    "        if e_m == 1:\n",
    "            raise ValueError(\"e_m = {}\".format(e_m))\n",
    "            \n",
    "        # 计算G_m的系数\n",
    "        alpha_m = None\n",
    "        # 更新训练数据集的权值分布\n",
    "        D = None\n",
    "        D = None\n",
    "        # 保存G_m和其系数\n",
    "        G.append(G_m)\n",
    "        alpha.append(alpha_m)\n",
    "    \n",
    "    # 构建基本分类器的线性组合\n",
    "    def F(X):\n",
    "        num_G = len(G)\n",
    "        score = 0\n",
    "        for i in range(num_G):\n",
    "            score += None\n",
    "        return None\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine (GBM)\n",
    "\n",
    "GBM和AdaBoost一样采用加法模型：$H(x) = \\sum_{t=1}^{T} \\alpha_t h_t(x)$，但GBM拓展为可以采用其他任意损失$l$（如前面介绍过的平方损失、交叉熵损失等）。\n",
    "\n",
    "GBM一般采用决策树（或回归树）作为基学习器，称为Gradient Boosting Decision Tree (GBDT)，针对不同问题使用不同的损失函数，分类问题使用指数损失函数，回归问题使用平方误差损失函数。\n",
    "\n",
    "GBDT的加法模型为:\n",
    "$$\n",
    "f_m(x) = \\sum_{m=1}^{M} T(x;\\Theta_m)\n",
    "$$\n",
    "其中$T(x;\\Theta_m)$表示决策树；$\\Theta_m$为决策树参数；M为树的个数。\n",
    "\n",
    "GBDT采用前向分步算法。首先确定初始提升树$f_0(x) = 0$，第m步的模型是\n",
    "$$\n",
    "f_m(x) = f_{m-1}(x) + T(x; \\Theta_m)\n",
    "$$\n",
    "其中，$f_{m-1}$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\\Theta_m$，\n",
    "$$\n",
    "\\hat{\\Theta}_m = \\arg \\underset{\\Theta_m}{\\min} \\sum_{i=1}^{N} L(y_i, f_{m-1}(x_i) + T(x_i; \\Theta_m))\n",
    "$$\n",
    "为了能够得到最优的下一棵决策树，Freidman提出了梯度提升（gradient boosting）算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值\n",
    "$$\n",
    "-\\left[ \\frac{\\partial L(y, f(x_i))}{\\partial f(x_i)} \\right]_{ f(x) = f_{m-1}(x) }\n",
    "$$\n",
    "作为回归问题提升树算法中的残差的近似值，拟合一个回归树。\n",
    "\n",
    "GBDT算法  \n",
    "输入：训练数据集$T=\\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}$，$x_i \\in \\mathcal{X} \\subseteq R^n$，$y_i \\in \\mathcal{Y} \\subseteq R$;  \n",
    "输出： 回归树$\\hat{y}(x)$。  \n",
    "（1）初始化\n",
    "$$\n",
    "f_0(x) = \\arg \\underset{c}{\\min}\\sum_{i=1}^{N}L(y_i, c)\n",
    "$$\n",
    "（2）对$m=1,2,\\cdots,M$  \n",
    "  (a)对$i=1,2,\\cdots,N$，计算\n",
    "$$\n",
    "r_{mi} = -\\left[ \\frac{\\partial L(y, f(x_i))}{\\partial f(x_i)} \\right]_{f(x) = f_{m-1}(x)}\n",
    "$$\n",
    "  (b)对$r_{mi}$拟合一个回归树，得到第m棵树的叶结点区域$R_{mj}$，$j=1,2,\\cdots,J$  \n",
    "  (c)对$j=1,2,\\cdots,J$，计算\n",
    "$$\n",
    "c_{mj} = \\arg \\underset{c}{\\min} \\sum_{x_i \\in R_{mj}} L(y_i, f_{m-1}(x_i) + c)\n",
    "$$\n",
    "  (d)更新$f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J} c_{mj}I(x \\in R_{mj})$  \n",
    "（3）得到回归树\n",
    "$$\n",
    "\\hat{f}(x) = f_M(x) = \\sum_{m=1}^{M} \\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})\n",
    "$$\n",
    "\n",
    "虽然说GBDT使用的是回归树，当是也可以用于分类问题，还记得Logistic Regression吗？逻辑回归解决的是二元分类问题，softmax可以解决多分类问题。如果损失函数$L(y, f(x_i))$为交叉熵损失，GBDT就可以解决分类问题，如果损失函数$L(y, f(x_i))$为平方差损失，GBDT就可以解决回归问题。\n",
    "\n",
    "$x_i$的平方差损失为\n",
    "$$\n",
    "L(y, f(x_i)) = \\frac{1}{2} (y_i - f(x_i))^2\n",
    "$$\n",
    "对应的$r_{mi}$为\n",
    "$$\n",
    "r_{mi} = -\\left[ \\frac{\\partial L(y, f(x_i))}{\\partial f(x_i)} \\right]_{f(x) = f_{m-1}(x)} = y - f_{m-1}(x)\n",
    "$$\n",
    "\n",
    "$x_i$的交叉熵损失为\n",
    "$$\n",
    "L(y, g(x_i)) = - y_i \\log (g(x_i)) - (1 - y_i) \\log (1 - g(x_i)) \n",
    "$$\n",
    "其中$g(x_i) = sigmoid(f(x_i))$，\n",
    "对应的$r_{mi}$为\n",
    "$$\n",
    "r_{mi} = -\\left[ \\frac{\\partial L(y, f(x_i))}{\\partial f(x_i)} \\right]_{f(x) = f_{m-1}(x)} = y - g(f_{m-1}(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **任务3：**根据上述描述构建GBDT算法\n",
    "\n",
    "**注意：**GBDT的弱学习器应采用回归模型，弱学习器采用scikit-learn的DecisionTreeRegressor实现，具体使用和之前的分类模型类似，可以参考[这里](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)，\n",
    "其中决策树的深度可由输入参数控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    计算sigmoid函数值\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def gbdt_classifier(X, y, M, max_depth=None):\n",
    "    \"\"\"\n",
    "    用于分类的GBDT函数\n",
    "    参数:\n",
    "        X: 训练样本\n",
    "        y: 样本标签，y = {0, +1}\n",
    "        M: 使用M个回归树\n",
    "        max_depth: 基学习器CART决策树的最大深度\n",
    "    返回:\n",
    "        F: 生成的模型\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # 用0初始化y_reg\n",
    "    y_reg = None\n",
    "    f = []\n",
    "    \n",
    "    for m in range(M):\n",
    "        # 计算r\n",
    "        r = None\n",
    "        \n",
    "        # 拟合r\n",
    "        # 使用DecisionTreeRegressor，设置树深度为5，random_state=0\n",
    "        f_m = None\n",
    "        # 开始训练\n",
    "        None\n",
    "        # 更新f\n",
    "        f.append(f_m)\n",
    "        \n",
    "        y_reg += None\n",
    "    \n",
    "    def F(X):\n",
    "        num_X, _ = X.shape\n",
    "        reg = np.zeros((num_X))\n",
    "        \n",
    "        for t in f:\n",
    "            reg += None\n",
    "        \n",
    "        y_pred_gbdt = sigmoid(reg)\n",
    "        \n",
    "        # 以0.5为阈值，得到最终分类结果0或1\n",
    "        one_position = None\n",
    "        y_pred_gbdt[one_position] = 1\n",
    "        y_pred_gbdt[~one_position] = 0\n",
    "        \n",
    "        return y_pred_gbdt\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，采用任务1实现CART的树策略作为弱学习器，即控制弱学习器CART树最大深度为5，分别构建adaboost和GBDT集成学习模型，弱学习器数目设置为10，查看最终结果。运行以下代码，可以看到adaboost和GBDT模型相较于一个CART决策树，准确率都得到了较大的提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用adaboost和GBDT模型进行训练\n",
    "\n",
    "adaboost_model = adaboost(X, y_ada, 10, max_depth=5)\n",
    "gbdt_model = gbdt_classifier(X, y, 10, max_depth=5)\n",
    "\n",
    "y_pre_ada = adaboost_model(X)\n",
    "y_pre_gbdt = gbdt_model(X)\n",
    "\n",
    "accuracy_ada = np.mean(y_pre_ada == y_ada)\n",
    "accuracy_gbdt = np.mean(y_pre_gbdt == y)\n",
    "\n",
    "print(\"Accuracy of Adaboost model is {}\".format(accuracy_ada))\n",
    "print(\"Accuracy of GBDT model is {}\".format(accuracy_gbdt))\n",
    "\n",
    "# 可视化CART的分类效果\n",
    "x1_grid = np.linspace(-2, 2, 100)\n",
    "x2_grid = np.linspace(-2, 2, 100)\n",
    "\n",
    "x1_grid, x2_grid = np.meshgrid(x1_grid, x2_grid)\n",
    "X_grid = np.hstack([x1_grid.reshape(-1, 1), x2_grid.reshape(-1, 1)])\n",
    "\n",
    "y_grid_ada = adaboost_model(X_grid)\n",
    "y_grid_gbdt = gbdt_model(X_grid)\n",
    "\n",
    "y_grid_simple_ada = y_grid_ada.reshape(100, 100)\n",
    "y_grid_simple_gbdt = y_grid_gbdt.reshape(100, 100)\n",
    "\n",
    "ada_fig=plt.figure()\n",
    "ax1=ada_fig.add_subplot(111)\n",
    "ax1.set_title('adaboost_classifier')\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y_ada)\n",
    "ax1.contourf(x1_grid, x2_grid, y_grid_simple_ada, alpha=0.3)\n",
    "\n",
    "gbdt_fig=plt.figure()\n",
    "ax2=gbdt_fig.add_subplot(111)\n",
    "ax2.set_title('gbdt_classifier')\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax2.contourf(x1_grid, x2_grid, y_grid_simple_gbdt, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging算法很简单，利用自助采样（有放回的均匀抽样）得到T组训练样本集，分别利用这些训练样本集训练T个分类器（CART or SVM or others），最后进行投票集成。\n",
    "\n",
    "从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更为明显。\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "随机森林是Bagging的一个扩展变体，它充分利用“随机”的思想来增加各分类器的多样性。“随机”体现在两个方面：基于自助采样法来选择训练样本和随机选择特征（或属性）。随机选择特征是指，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性和引入程度，一般情况下，推荐值$k=\\log_2d$(假设有d个属性)。随机森林的弱分类器一般是CART。随机森林的特点是可高度并行化、继承了CART的优点和克服了完全生长树的缺点。\n",
    "\n",
    "Scikit-learn实现的CART算法默认随机选择特征，因此，直接采用bagging算法集成CART树就是Random Forest的实现函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **任务4：**构建bagging算法\n",
    "\n",
    "采用投票法作为最终集成的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(X, y, T, size, seed=0, max_depth=None):\n",
    "    \"\"\"\n",
    "    Bagging算法，分类器为CART，用于二分类\n",
    "    参数：\n",
    "        X: 训练集\n",
    "        y: 样本标签\n",
    "        T: T组\n",
    "        size: 每组训练集的大小\n",
    "        seed: 随机种子\n",
    "        max_depth: 基学习器CART决策树的最大深度\n",
    "    返回：\n",
    "        F: 生成的模型\n",
    "    \"\"\"\n",
    "    classifiers = []\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    np.random.seed(seed)\n",
    "    for i in range(T):\n",
    "        # 使用np.random.choice选择size个序号，注意replace参数的设置，以满足有放回的均匀抽样。\n",
    "        index = None\n",
    "        X_group = X[index]\n",
    "        y_group = y[index]\n",
    "        # 使用tree.DecisionTreeClassifier，设置max_depth=max_depth, min_samples_split=2(生成完全树),random_state=0\n",
    "        t = None\n",
    "        # 开始训练\n",
    "        None\n",
    "        classifiers.append(t)\n",
    "    \n",
    "    def F(X):\n",
    "        # 计算所有分类器的预测结果\n",
    "        result = []\n",
    "        for t in classifiers:\n",
    "            None\n",
    "        # 把预测结果组成 num_X * T 的矩阵\n",
    "        pred = None\n",
    "        # 计算\"0\"有多少投票\n",
    "        vote_0 = None\n",
    "        # 计算\"1\"有多少投票\n",
    "        vote_1 = None\n",
    "        # 选择投票数最多的一个标签\n",
    "        pred = None\n",
    "        \n",
    "        return pred     \n",
    "    ### END CODE HERE ###\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，三大集成学习算法都已经实现了。集成学习这一章节结束，我们已经学习了大部分的经典的监督学习算法。下面我们考虑完成一个基础的机器学习应用。\n",
    "\n",
    "## Titanic: Machine Learning from Disaster\n",
    "\n",
    "Titanic生存预测是kaggle竞赛机器学习入门的经典题目，Kaggle提供的数据集中，共有1309名乘客数据，其中891是已知存活情况，剩下418则是需要进行分析预测的。我们采用其提供的训练数据titanic_train.csv来进行本次实验。\n",
    "\n",
    "首先来看一下titanic_train.csv共有891条数据，包含一下内容：\n",
    "\n",
    "PassengerId: 乘客编号  \n",
    "Survived   : 存活情况（存活：1 ; 死亡：0）  \n",
    "Pclass     : 客舱等级  \n",
    "Name       : 乘客姓名  \n",
    "Sex        : 性别  \n",
    "Age        : 年龄  \n",
    "SibSp      : 同乘的兄弟姐妹/配偶数  \n",
    "Parch      : 同乘的父母/小孩数  \n",
    "Ticket     : 船票编号  \n",
    "Fare       : 船票价格  \n",
    "Cabin      : 客舱号  \n",
    "Embarked   : 登船港口  \n",
    "\n",
    "PassengerId :   891 non-null int64  \n",
    "Survived    :   891 non-null int64  \n",
    "Pclass      :   891 non-null int64  \n",
    "Name        :   891 non-null object  \n",
    "Sex         :   891 non-null object  \n",
    "Age         :   714 non-null float64  \n",
    "SibSp       :   891 non-null int64  \n",
    "Parch       :   891 non-null int64  \n",
    "Ticket      :   891 non-null object  \n",
    "Fare        :   891 non-null float64  \n",
    "Cabin       :   204 non-null object  \n",
    "Embarked    :   889 non-null object  \n",
    "\n",
    "在实际的机器学习任务中，数据处理应占整个项目相当大的比例，怎样从原始数据中心得到有效的信息，对于建模及其关键。Titanic数据集中有部分信息是缺失的，如何选取有效字段进行保留，如何补全缺失值，使我们在进行建模前需要考虑的问题。\n",
    "这里提供一个Titanic数据集比较全面的数据分析过程，大家可以从中学习和了解机器学习数据清洗的方法。[链接](https://www.kaggle.com/startupsci/titanic-data-science-solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **任务5：**处理Titanic训练数据\n",
    "\n",
    "采用pandas工具进行数据分析[这里](https://pandas.pydata.org/pandas-docs/stable/reference/index.html)可以查看其官方文档。\n",
    "\n",
    "本次作业我们采用较为简单的方式处理该数据集，去掉姓名，乘客编号和缺失值较多的客舱号三项；采用均值来填满缺失的年龄信息；将客舱等级、性别和登船港口三列转为离散值进行处理；合并SibSp和Parch得到总的家人数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./titanic/train.csv\")\n",
    "\n",
    "### START CODE HERE ###\n",
    "# 采用mean()得到年龄均值，填补缺失信息\n",
    "average_age = None\n",
    "data_train.loc[(data_train.Age.isnull()), 'Age' ] = average_age \n",
    "\n",
    "# 采用pd.get_dummies得到离散数据\n",
    "dummies_Embarked = None\n",
    "dummies_Sex = None\n",
    "dummies_Pclass = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "# 采用pd.oncat合并原始数据和生成的离散数据\n",
    "df = pd.concat([data_train, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df['family_num'] = df['SibSp'].values + df['Parch'].values\n",
    "\n",
    "# 得到最终的训练数据的字段\n",
    "train_df = df.filter(regex='Survived|Age.*|family_num|Fare.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "# 显示最终处理完成的数据信息\n",
    "print(train_df.info())\n",
    "\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证试验\n",
    "\n",
    "采用不同的模型进行训练，包括CART树、adaboost、GBDT和Random Forest算法，其中树的最大深度都固定为10。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits)\n",
    "accuracy_train_ada, accuracy_val_ada = 0, 0\n",
    "accuracy_train_gbdt, accuracy_val_gbdt = 0, 0\n",
    "accuracy_train_rf, accuracy_val_rf = 0, 0\n",
    "accuracy_train_CART, accuracy_val_CART = 0, 0\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    y_train_ada = y_train.copy()\n",
    "    y_train_ada[y_train_ada == 0] = -1\n",
    "    y_test_ada = y_test.copy()\n",
    "    y_test_ada[y_test_ada == 0] = -1\n",
    "    \n",
    "    adaboost_model = adaboost(X_train, y_train_ada, 100, max_depth=10)\n",
    "    gbdt_model = gbdt_classifier(X_train, y_train, 100, max_depth=10)\n",
    "    randomforest_model = bagging(X_train, y_train, 100, int(X_train.shape[0]*0.4), max_depth=10)\n",
    "    CART = DecisionTreeClassifier(max_depth=10)\n",
    "    CART.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pre_ada = adaboost_model(X_train)\n",
    "    y_test_pre_ada = adaboost_model(X_test)\n",
    "    y_train_pre_gbdt = gbdt_model(X_train)\n",
    "    y_test_pre_gbdt = gbdt_model(X_test)\n",
    "    y_train_pre_rf = randomforest_model(X_train)\n",
    "    y_test_pre_rf = randomforest_model(X_test)\n",
    " \n",
    "    accuracy_train_CART += CART.score(X_train, y_train)\n",
    "    accuracy_val_CART += CART.score(X_test, y_test)\n",
    "    accuracy_train_ada += np.mean(y_train_pre_ada == y_train_ada)\n",
    "    accuracy_val_ada += np.mean(y_test_pre_ada == y_test_ada)\n",
    "    accuracy_train_gbdt += np.mean(y_train_pre_gbdt == y_train)\n",
    "    accuracy_val_gbdt += np.mean(y_test_pre_gbdt == y_test)\n",
    "    accuracy_train_rf += np.mean(y_train_pre_rf == y_train)\n",
    "    accuracy_val_rf += np.mean(y_test_pre_rf == y_test)\n",
    "    \n",
    "    \n",
    "print(\"Accuracy of cart model in trainset is {}, in validation set is {}\".format(accuracy_train_CART/n_splits, accuracy_val_CART/n_splits))\n",
    "print(\"Accuracy of adaboost model in trainset is {}, in validation set is {}\".format(accuracy_train_ada/n_splits, accuracy_val_ada/n_splits))\n",
    "print(\"Accuracy of gbdt model in trainset is {}, in validation set is {}\".format(accuracy_train_gbdt/n_splits, accuracy_val_gbdt/n_splits))\n",
    "print(\"Accuracy of random forest model in trainset is {}, in validation set is {}\".format(accuracy_train_rf/n_splits, accuracy_val_rf/n_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察上述结果，我们可以发现AdaBoost和GBDT模型主要关注降低错误率（即降低偏差），因此他们能基于分类性能相当弱的学习器构建出分类性能很强的分类器；Bagging主要关注降低方差，因此Random Forest得到的结果在验证集上表现较好，不容易过拟合。\n",
    "\n",
    "到这里我们本次作业就结束了，有兴趣的同学可以用我们生成的模型去拟合kaggle测试集的数据，提交结果得到最终成绩。\n",
    "\n",
    "当然这个模型还有很大的提升空间，首先是数据方面，采用更高级的方式补充缺失值，保留姓名字段等方式都可以得到更充足的数据信息，其次模型选择，弱学习器的选择，一些超参数的调节，这些都会对最终的模型产生影响，你也可以采用上述策略去提升kaggle竞赛的最终成绩。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
